---
title: 쿠버네티스 02 - 관리되는 POD
date: 2023-12-22
categories: ["KUBENETES", "BASIC"]
tags: ['kubenetes']     # TAG names should always be lowercase
toc: true
---

사실 Pod을 직접 실행하면, 해당 Pod은 k8s에서 관리하지 않는다. 중간에 멈추거나 이상이 생겨도 새로운 Pod을 k8s에서 생성해주지 않는다. k8s에서 Pod을 관리하며 안정적으로 유지할 수 있는 방법을 알아보자.

## LivenessProbe

Pod의 `spec`에 명시하여서 Pod의 `외부`에서 Pod의 상태를 관찰할 수 있게 해준다. 크게 세 가지 방법이 존재한다.

1. HTTP GET
    
    가장 기본적인 방법이다. Pod의 컨테이너에 일정한 Path에 GET 요청을 보내고, 성공코드(2xx 혹은 3xx)면 정상상태인것으로 간주한다.

2. TCP 소켓

    컨테이너의 지정된 포트에 TCP 연결을 시도하고, 성공하면 정상상태인것으로 간주한다.

3. Exec

    컨테이너 내의 임의의 명령을 실행하고 명령의 종료 상태 코드를 확인한다. (0이면 정상상태)

### HTTP 기반 LivenessProbe 생성하기
```yaml
apiVersion: v1
kind: Pod
metadata:
    name: liveness-probe
spec:
    containers:
    - image: nginx/nginx
      name: nginx
      livenessProbe:
      httpGet:
        path: /
        port: 8080
```

> 컨테이너가 문제가 있다고 판단되면 컨테이너를 재시작시킨다. 이전 컨테이너의 로그를 확인하기 위해서는 `kubectl logs <pod_name> --previous`를 사용한다.
{: .prompt-info }

컨테이너가 초기에 실행되고 일정 시간이 필요할 때도 있다. 그럴땐 `liveness-probe.initialDelaySeconds` 옵션을 사용하자.

> LivenessProbe 작업은 파드를 호스트하는 노드의 Kubelet에서 이루어진다. 즉, 노드 자체에 문제가 생겨서 파드에 문제가 전파될 시에는 작동하지 않는다.
{: .prompt-warning }

그렇다면 노드 자체에 문제가 생겨도 다른 노드에서 파드를 실행시키고 관리할 수 있는 방법은 뭐가 있을까?

## ReplicationController

> 레플리케이션컨트롤러는 디플로이, 레플리카셋으로 대체하는 것이 좋다.
{: .prompt-info }

레플리케이션컨트롤러는 특정 레이블을 가지고 있는 파드를 모니터링하며, 그 수가 의도하는 수와 일치하는지 확인하고 일치하지 않는다면 일치하게 유지한다. 레플리케이션컨트롤러에는 세 가지의 필수 요소가 있다.
1. 레이블 셀렉터: 어떤 레이블을 가지고 있는 파드를 관리할 것인지.
2. 레플리카 카운트: 그 파드를 몇개를 유지하고 싶은지.
3. 파드 템플릿: 해당 파드는 어떻게 구성되어 있는지.

이 세가지의 요소중에 `레플리카 카운트`만 기존 파드에 영향을 준다. 즉, 레이블 셀렉터를 변경하면, 기존 파드는 그대로 유지되는 채로 새로운 파드를 생성한다. 하지만 파드 템플릿의 레이블을 변경하지 않았으므로 파드가 무한정 생성될 여지가 있다(사실 애초에 이렇게는 생성이 불가능하다). 

#### 작동원리
작동원리는 생각보다 간단하다. 만약에 레플리케이션 컨트롤러가 관리중인 Pod가 삭제되면, 레플리케이션 컨트롤러는 이 사실을 `통지`받는다(통지 자체가 파드를 생성하진 않는다). 통지는 해당 컨트롤러가 실제 Pod수를 확인하고 조치를 취하는 트리거 역할을 할뿐이다.

## ReplicaSet

차세대 레플리케이션 컨트롤러이며, 레플리케이션컨트롤러를 완전히 대체할 수 있는 리소스다. 앞에서 살펴본 Replication Controller는 특정 레이블이 있는 파드만을 매칭시킬 수 있지만 레플리카셋은 이보다 풍부한 레이블 조건을 사용할 수 있다.
```yaml
apiVersion: v1
kind: ReplicaSet
metadata:
    name: nginx
spec:
    replicas: 3
    selector:
        matchLabels:
            app: nginx
        matchExpressions:
            - key: app
              operator: In # NotIn, Exists, DoesNotExist 등등
              values:
                - kangho
    template:
        ...replicaionController와 동일...
```

## DaemonSet

레플리카셋은 k8s 클러스터 내 어딘가에 지정된 수만큼의 파드를 랜덤하게 생성한다. 하지만 데몬셋을 이용하면 모든 노드에 하나씩의 파드를 실행시킬 수 있다.

> 데몬셋에 의해 생성되는 파드는 타깃노드가 이미 정해져 있고, 쿠버네티스 스케쥴러를 건너뛴다.
{: .prompt-info }

```yaml
apiVersion: v1
kind: DaemonSet
metadata:
    name: ssd-monitor
spec:
    selector:
        matchLabels:
            app: ssd-monitor
    template:
        metadata:
            labels:
                app: ssd-monitor
        spec:
            nodeSelector:
                disk: ssd
        ...
```

disk=ssd를 레이블로 가지고 있는 노드에 하나씩 파드가 생성된다.

## Completable task 수행하는 Pod

위의 리소스들은 계속 실행되는 task를 실행한다. 즉, pod 내부의 컨테이너의 task가 끝내버려서 종료되면 다시 생성시킨다. Job 리소스는 pod 내부의 실행 중인 프로세스가 성공적으로 완료되면 컨테이너를 다시 시작하지 않는 Pod를 생성할 수 있다.

#### Job

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never 
  backoffLimit: 4 # 재시도 가능 횟수
  activeDeadlineSeconds: 20 # 실행시간 제한
```
> Job의 Pod은 `restartPolicy: Always`를 가질 수 없다.
{: .prompt-warning }

#### CronJob

주기적으로 실행되는 Job이다.
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure

```